{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation in DeepPavlov\n",
    "Learn how to read and prepare data for trainable components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Deeppavlov library has functionality to download and decompress the data. For this purpose the `download_decompress` from `data.utils` is used. \n",
    "The following cell will download the CoNLL-2003 data for the Named Entity Recognition (NER) task and put it to the folder `data/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import deeppavlov\n",
    "from deeppavlov.core.data.utils import download_decompress\n",
    "download_decompress('http://files.deeppavlov.ai/deeppavlov_data/conll2003_v2.tar.gz', 'data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing text data into a machine-readable dataset \n",
    "\n",
    "We will work with a corpus which contains tweets with NE tags. A typical file with NER data contains lines with pairs of tokens (word or punctuation symbol) and tags separated by a whitespace. In many cases additional information such as POS-tags is included. \n",
    "\n",
    "Different documents are separated by lines **started** with **-DOCSTART-** token. Different sentences are separated by an empty line. Example:\n",
    "\n",
    "    -DOCSTART- -X- -X- O\n",
    "\n",
    "    EU NNP B-NP B-ORG\n",
    "    rejects VBZ B-VP O\n",
    "    German JJ B-NP B-MISC\n",
    "    call NN I-NP O\n",
    "    to TO B-VP O\n",
    "    boycott VB I-VP O\n",
    "    British JJ B-NP B-MISC\n",
    "    lamb NN I-NP O\n",
    "    . . O O\n",
    "\n",
    "    Peter NNP B-NP B-PER\n",
    "    Blackburn NNP I-NP I-PER\n",
    "\n",
    "In this tutorial we will focus only on tokens and tags (first and last elements of the line) and drop POS information located between them.\n",
    "\n",
    "We start by building a class *NerDatasetReader*  that provides functionality for reading the dataset. It returns a dictionary with fields *train*, *test*, and *valid*. Each field stores a list of samples. Each sample is a tuple of tokens and tags. Both tokens and tags are lists. The following example depicts the structure that should be returned by *read* method:\n",
    "\n",
    "    {'train': [(['Mr.', 'Dwag', 'are', 'derping', 'around'], ['B-PER', 'I-PER', 'O', 'O', 'O']), ....],\n",
    "     'valid': [...],\n",
    "     'test': [...]}\n",
    "\n",
    "There are three separate parts in the dataset:\n",
    " - *train* data for training the model;\n",
    " - *validation* data for evaluation and hyperparameters tuning;\n",
    " - *test* data for final evaluation of the model.\n",
    " \n",
    "\n",
    "Each of these parts is stored in a separate txt file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "class NerDatasetReader:\n",
    "    def read(self, data_path):\n",
    "        data_parts = ['train', 'valid', 'test']\n",
    "        extension = '.txt'\n",
    "        dataset = {}\n",
    "        for data_part in data_parts:\n",
    "            file_path = Path(data_path) / Path(data_part + extension)\n",
    "            dataset[data_part] = self.read_file(str(file_path))\n",
    "        return dataset\n",
    "            \n",
    "    @staticmethod\n",
    "    def read_file(file_path):\n",
    "        \n",
    "        # Use utf-8 encoding when open the file\n",
    "        ######################################\n",
    "        ########## YOUR CODE HERE ############\n",
    "        ######################################\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_reader = NerDatasetReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset_reader.read('data/')\n",
    "assert len(dataset) == 3, 'The dataset must be a dict with three fields: train, test, and valid'\n",
    "assert len(set(dataset) & {'train', 'test', 'valid'}) == 3, 'The dataset keys must be exactly train, test, and valid'\n",
    "assert isinstance(dataset['train'][0][0][0], str) and isinstance(dataset['train'][0][0][1], str), 'Both tokens and tags must be strings'\n",
    "assert len(dataset['train']) == 14041, 'there must be exactly 14041 samples in train'\n",
    "assert len(dataset['valid']) == 3250, 'there must be exactly 3250 samples in train'\n",
    "assert len(dataset['test']) == 3453, 'there must be exactly 3453 samples in test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should always understand what kind of data you deal with. For this purpose, you can print the data by running the code in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for sample in dataset['train'][:2]:\n",
    "    for token, tag in zip(*sample):\n",
    "        print('%s\\t%s' % (token, tag))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find an implementation of the dataset reader that implemets the same interfaces in the library: [Conll2003DatasetReader](https://github.com/deepmipt/DeepPavlov/blob/dev/deeppavlov/dataset_readers/conll2003_reader.py). The functionality of the presented code is wider and the `register` wrapper allows to use this component as a part of config file (will be discussed later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dictionaries\n",
    "\n",
    "To train a neural network, we will use two mappings: \n",
    "- {token}$\\to${token id}: address the row in embeddings matrix for the current token;\n",
    "- {tag}$\\to${tag id}: one-hot ground truth probability distribution vectors for computing the loss at the output of the network.\n",
    "\n",
    "Token indices will be used to address a row in embeddings matrix. The mapping for tags will be used to create one-hot ground-truth probability distribution vectors to compute the loss at the output of the network.\n",
    "\n",
    "Now you need to implement the *Vocab* class which will return {token or tag}$\\to${index} and vice versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from itertools import chain\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self,\n",
    "                 special_tokens=tuple()):\n",
    "        self.special_tokens = special_tokens\n",
    "        self._t2i = defaultdict(lambda: 1)\n",
    "        self._i2t = []\n",
    "        \n",
    "    def fit(self, tokens):\n",
    "        count = 0\n",
    "        self.freqs = Counter(chain(*tokens))\n",
    "        # The first special token will be the default token\n",
    "        for special_token in self.special_tokens:\n",
    "            self._t2i[special_token] = count\n",
    "            self._i2t.append(special_token)\n",
    "            count += 1\n",
    "        for token, freq in self.freqs.most_common():\n",
    "            if token not in self._t2i:\n",
    "                self._t2i[token] = count\n",
    "                self._i2t.append(token)\n",
    "                count += 1\n",
    "\n",
    "    def __call__(self, batch, **kwargs):\n",
    "        # Implement the vocab() method. The input could be a batch of tokens\n",
    "        # or a batch of indices. A batch is a list of utterances where each\n",
    "        # utterance is a list of tokens\n",
    "        pass\n",
    "        ######################################\n",
    "        ########## YOUR CODE HERE ############\n",
    "        ######################################\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        # Implement the vocab[] method. The input could be a token\n",
    "        # (string) or an index. You have to detect what type of data\n",
    "        # is key and return. \n",
    "        pass\n",
    "        ######################################\n",
    "        ########## YOUR CODE HERE ############\n",
    "        ######################################\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._i2t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing the function *build_dict* you can make dictionaries for tokens and tags. Special tokens in our case will be:\n",
    " - `<UNK>` token for out of vocabulary tokens\n",
    " - `'O'` for the tag vocab to place out of label tag to the first place with index 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "special_tokens = ['<UNK>']\n",
    "special_tags = ['O']\n",
    "\n",
    "token_vocab = Vocab(special_tokens)\n",
    "tag_vocab = Vocab(special_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will fit the vocabularies on the *train* part of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_tokens_by_sentenses = [tokens for tokens, tags in dataset['train']]\n",
    "all_tags_by_sentenses = [tags for tokens, tags in dataset['train']]\n",
    "\n",
    "token_vocab.fit(all_tokens_by_sentenses)\n",
    "tag_vocab.fit(all_tags_by_sentenses)\n",
    "\n",
    "assert len(token_vocab) == 23624, 'There must be exactly 23624 in the token vocab!'\n",
    "assert len(tag_vocab) == 9, 'There must be exactly 9 in the tag vocab!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to get the indices. Keep in mind that we are working with batches of the following structure:\n",
    "    \n",
    "    [['utt0_tok0', 'utt1_tok1', ...], ['utt1_tok0', 'utt1_tok1', ...], ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices_batch = token_vocab([['How', 'to', 'cook', 'a', 'turnip', '?']])\n",
    "\n",
    "assert len(indices_batch) == 1, 'the batch length must be 1'\n",
    "assert isinstance(indices_batch[0][0], int), 'The batch must contain lists of ints!'\n",
    "\n",
    "print(indices_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_indices_batch = tag_vocab([['O', 'O', 'O'], ['B-PER']])\n",
    "\n",
    "assert len(tag_indices_batch) == 2, 'the batch length must be 2'\n",
    "assert isinstance(tag_indices_batch[0][0], int), 'The batch must contain lists of ints!'\n",
    "\n",
    "print(tag_indices_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try converting from indices to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_vocab([np.random.randint(0, 512, size=10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar vocabulary is already implemented in the [library](https://github.com/deepmipt/DeepPavlov/blob/dev/deeppavlov/core/data/simple_vocab.py). It has extended functionality:\n",
    "- token cutoff by frequency\n",
    "- limitation of the vocabulary size\n",
    "- saving and loading\n",
    "- dict like dunders (\\_\\_contain\\_\\_, \\_\\_len\\_\\_, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Iterator\n",
    "\n",
    "Neural Networks are usually trained with batches. It means that weight updates of the network are based on several sequences at every single time. You have to iterate over the dataset and generate `x` and `y` batch by batch. The batch of `x`-s is a list of sentences of tokens like\n",
    "\n",
    "    [['Yan', 'is', 'a', 'good', 'fellow],\n",
    "     ['For', 'instance']]\n",
    "\n",
    "and the tag sequence should be:\n",
    "\n",
    "    [['B-PER', 'O', 'O', 'O', 'O'],\n",
    "     ['O', 'O']]\n",
    "\n",
    "An important concept in the batch generation is shuffling. Shuffling is taking sample from the dataset at random order. It is important to train on the shuffled data because large number consequetive samples of the same class may result in pure quality of the model.\n",
    "    \n",
    "The idea behind the iterator is to perform computation in the lazy way. Use yield generator expression to do so. An example of using yield for generator creation is provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterator():\n",
    "    data = [1, 2, 3]\n",
    "    for d in data:\n",
    "        yield d\n",
    "            \n",
    "print(iterator)\n",
    "    \n",
    "for i in iterator():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the `DatasetIterator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DatasetIterator:\n",
    "    def __init__(self, data):\n",
    "        self.data = {\n",
    "            'train': data['train'],\n",
    "            'valid': data['valid'],\n",
    "            'test': data['test']\n",
    "        }\n",
    "\n",
    "    def gen_batches(self, batch_size, data_type='train', shuffle=True):\n",
    "        ######################################\n",
    "        ########## YOUR CODE HERE ############\n",
    "        ######################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataset iterator from the loaded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_iterator = DatasetIterator(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, y = next(data_iterator.gen_batches(2))\n",
    "\n",
    "assert len(x) == 2, 'There must be two examples in the batch!'\n",
    "assert len(y) == 2, 'There must be two examples in the batch!'\n",
    "assert len(x[0]) == len(y[0]), 'The numbers of tokens and tags are different!'\n",
    "assert isinstance(x[0][0], str), 'Token must be a string!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a typical part of the data preprocessing pipeline. This parts will be used in the following tutorials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
