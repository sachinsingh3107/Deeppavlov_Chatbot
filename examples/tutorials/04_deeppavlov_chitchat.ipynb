{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can also run the notebook in [COLAB](https://colab.research.google.com/github/deepmipt/DeepPavlov/blob/master/examples/tutorials/04_deeppavlov_chitchat.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install deeppavlov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepPavlov sequence-to-sequence tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we are going to implement sequence-to-sequence [[original paper]](https://arxiv.org/abs/1409.3215) model in DeepPavlov.\n",
    "\n",
    "Sequence-to-sequence is the concept of mapping input sequence to target sequence. Sequence-to-sequence models consist of two main components: encoder and decoder. Encoder is used to encode the input sequence to dense representation and decoder uses this dense representation to generate target sequence.\n",
    "\n",
    "![sequence-to-sequence](img/seq2seq.png)\n",
    "\n",
    "Here, input sequence is ABC, special token <EOS\\> (end of sequence) is used as indicator to start decoding target sequence WXYZ.\n",
    "\n",
    "To implement this model in DeepPavlov we have to code some DeepPavlov abstractions:\n",
    "* **DatasetReader** to read the data\n",
    "* **DatasetIterator** to generate batches\n",
    "* **Vocabulary** to convert words to indexes\n",
    "* **Model** to train it and then use it\n",
    "* and some other components for pre- and postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import deeppavlov\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from itertools import chain\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download & extract dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.data.utils import download_decompress\n",
    "download_decompress('http://files.deeppavlov.ai/datasets/personachat_v2.tar.gz', './personachat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DatasetReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DatasetReader is used to read and parse data from files. Here, we define new PersonaChatDatasetReader which reads [PersonaChat dataset](https://arxiv.org/abs/1801.07243). PersonaChat dataset consists of dialogs and user personalities.\n",
    "\n",
    "User personality is described by four sentences, e.g.:\n",
    "\n",
    "    i like to remodel homes.\n",
    "    i like to go hunting.\n",
    "    i like to shoot a bow.\n",
    "    my favorite holiday is halloween."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov import build_model\n",
    "from deeppavlov.core.data.dataset_reader import DatasetReader\n",
    "from deeppavlov.core.data.utils import download_decompress\n",
    "from deeppavlov.core.common.registry import register\n",
    "\n",
    "@register('personachat_dataset_reader')\n",
    "class PersonaChatDatasetReader(DatasetReader):\n",
    "    \"\"\"\n",
    "    PersonaChat dataset from\n",
    "    Zhang S. et al. Personalizing Dialogue Agents: I have a dog, do you have pets too?\n",
    "    https://arxiv.org/abs/1801.07243\n",
    "    Also, this dataset is used in ConvAI2 http://convai.io/\n",
    "    This class reads dataset to the following format:\n",
    "    [{\n",
    "        'persona': [list of persona sentences],\n",
    "        'x': input utterance,\n",
    "        'y': output utterance,\n",
    "        'dialog_history': list of previous utterances\n",
    "        'candidates': [list of candidate utterances]\n",
    "        'y_idx': index of y utt in candidates list\n",
    "      },\n",
    "       ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    def read(self, dir_path: str, mode='self_original'):\n",
    "        dir_path = Path(dir_path)\n",
    "        dataset = {}\n",
    "        for dt in ['train', 'valid', 'test']:\n",
    "            dataset[dt] = self._parse_data(dir_path / '{}_{}.txt'.format(dt, mode))\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_data(filename):\n",
    "        examples = []\n",
    "        print(filename)\n",
    "        curr_persona = []\n",
    "        curr_dialog_history = []\n",
    "        persona_done = False\n",
    "        with filename.open('r') as fin:\n",
    "            for line in fin:\n",
    "                line = ' '.join(line.strip().split(' ')[1:])\n",
    "                your_persona_pref = 'your persona: '\n",
    "                if line[:len(your_persona_pref)] == your_persona_pref and persona_done:\n",
    "                    curr_persona = [line[len(your_persona_pref):]]\n",
    "                    curr_dialog_history = []\n",
    "                    persona_done = False\n",
    "                elif line[:len(your_persona_pref)] == your_persona_pref:\n",
    "                    curr_persona.append(line[len(your_persona_pref):])\n",
    "                else:\n",
    "                    persona_done = True\n",
    "                    x, y, _, candidates = line.split('\\t')\n",
    "                    candidates = candidates.split('|')\n",
    "                    example = {\n",
    "                        'persona': curr_persona,\n",
    "                        'x': x,\n",
    "                        'y': y,\n",
    "                        'dialog_history': curr_dialog_history[:],\n",
    "                        'candidates': candidates,\n",
    "                        'y_idx': candidates.index(y)\n",
    "                    }\n",
    "                    curr_dialog_history.extend([x, y])\n",
    "                    examples.append(example)\n",
    "\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PersonaChatDatasetReader().read('./personachat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's check dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in data:\n",
    "    print(k, len(data[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset iterator is used to generate batches from parsed dataset (DatasetReader). Let's extract only *x* and *y* from parsed dataset and use them to predict sentence *y* by sentence *x*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.data.data_learning_iterator import DataLearningIterator\n",
    "\n",
    "@register('personachat_iterator')\n",
    "class PersonaChatIterator(DataLearningIterator):\n",
    "    def split(self, *args, **kwargs):\n",
    "        for dt in ['train', 'valid', 'test']:\n",
    "            setattr(self, dt, self._to_tuple(getattr(self, dt)))\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_tuple(data):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            list of (x, y)\n",
    "        \"\"\"\n",
    "        return list(map(lambda x: (x['x'], x['y']), data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look on data in batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = PersonaChatIterator(data)\n",
    "batch = [el for el in iterator.gen_batches(5, 'train')][0]\n",
    "for x, y in zip(*batch):\n",
    "    print('x:', x)\n",
    "    print('y:', y)\n",
    "    print('----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer is used to extract tokens from utterance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.models.tokenizers.lazy_tokenizer import LazyTokenizer\n",
    "tokenizer = LazyTokenizer()\n",
    "tokenizer(['Hello my friend'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary prepares mapping from tokens to token indexes. It uses train data to build this mapping.\n",
    "\n",
    "We will implement DialogVocab (inherited from SimpleVocabulary) wich adds all tokens from *x* and *y* utterances to vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.data.simple_vocab import SimpleVocabulary\n",
    "\n",
    "@register('dialog_vocab')\n",
    "class DialogVocab(SimpleVocabulary):\n",
    "    def fit(self, *args):\n",
    "        tokens = chain(*args)\n",
    "        super().fit(tokens)\n",
    "\n",
    "    def __call__(self, batch, **kwargs):\n",
    "        indices_batch = []\n",
    "        for utt in batch:\n",
    "            tokens = [self[token] for token in utt]\n",
    "            indices_batch.append(tokens)\n",
    "        return indices_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create instance of DialogVocab. We define save and load paths, minimal frequence of tokens which are added to vocabulary and set of special tokens.\n",
    "\n",
    "Special tokens are:\n",
    "* <PAD\\> - padding\n",
    "* <BOS\\> - begin of sequence\n",
    "* <EOS\\> - end of sequence\n",
    "* <UNK\\> - unknown token - token which is not presented in vocabulary\n",
    "\n",
    "And fit it on tokens from *x* and *y*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = DialogVocab(\n",
    "    save_path='./vocab.dict',\n",
    "    load_path='./vocab.dict',\n",
    "    min_freq=2,\n",
    "    special_tokens=('<PAD>','<BOS>', '<EOS>', '<UNK>',),\n",
    "    unk_token='<UNK>'\n",
    ")\n",
    "\n",
    "vocab.fit(tokenizer(iterator.get_instances(data_type='train')[0]), tokenizer(iterator.get_instances(data_type='train')[1]))\n",
    "vocab.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 most frequent tokens in train dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of tokens in vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use built vocabulary to encode some tokenized sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab([['<BOS>', 'hello', 'my', 'friend', 'there_is_no_such_word_in_dataset', 'and_this', '<EOS>', '<PAD>']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To feed sequences of token indexes to neural model we should make their lengths equal. If sequence is too short we add <PAD\\> symbols to the end of sequence. If sequence is too long we just cut it.\n",
    "\n",
    "SentencePadder implements such behavior, it also adds <BOS\\> and <EOS\\> tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.models.component import Component\n",
    "\n",
    "@register('sentence_padder')\n",
    "class SentencePadder(Component):\n",
    "    def __init__(self, length_limit, pad_token_id=0, start_token_id=1, end_token_id=2, *args, **kwargs):\n",
    "        self.length_limit = length_limit\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.start_token_id = start_token_id\n",
    "        self.end_token_id = end_token_id\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        for i in range(len(batch)):\n",
    "            batch[i] = batch[i][:self.length_limit]\n",
    "            batch[i] = [self.start_token_id] + batch[i] + [self.end_token_id]\n",
    "            batch[i] += [self.pad_token_id] * (self.length_limit + 2 - len(batch[i]))\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padder = SentencePadder(length_limit=6)\n",
    "vocab(padder(vocab([['hello', 'my', 'friend', 'there_is_no_such_word_in_dataset', 'and_this']])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq Model\n",
    "Model consists of two main components: encoder and decoder. We can implement them independently and then put them together in one Seq2Seq model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder\n",
    "Encoder builds hidden representation of input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(inputs, inputs_len, embedding_matrix, cell_size, keep_prob=1.0):\n",
    "    # inputs: tf.int32 tensor with shape bs x seq_len with token ids\n",
    "    # inputs_len: tf.int32 tensor with shape bs\n",
    "    # embedding_matrix: tf.float32 tensor with shape vocab_size x vocab_dim\n",
    "    # cell_size: hidden size of recurrent cell\n",
    "    # keep_prob: dropout keep probability\n",
    "    with tf.variable_scope('encoder'):\n",
    "        # first of all we should embed every token in input sequence (use tf.nn.embedding_lookup, don't forget about dropout)\n",
    "        x_emb = tf.nn.dropout(tf.nn.embedding_lookup(embedding_matrix, inputs), keep_prob=keep_prob)\n",
    "        \n",
    "        # define recurrent cell (LSTM or GRU)\n",
    "        encoder_cell = tf.nn.rnn_cell.GRUCell(\n",
    "                            num_units=cell_size,\n",
    "                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                            name='encoder_cell')\n",
    "        \n",
    "        # use tf.nn.dynamic_rnn to encode input sequence, use actual length of input sequence\n",
    "        encoder_outputs, encoder_state = tf.nn.dynamic_rnn(cell=encoder_cell, inputs=x_emb, sequence_length=inputs_len, dtype=tf.float32)\n",
    "    return encoder_outputs, encoder_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your encoder implementation:\n",
    "\n",
    "next cell output shapes are\n",
    "\n",
    "32 x 10 x 100 and 32 x 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "vocab_size = 100\n",
    "hidden_dim = 100\n",
    "inputs = tf.cast(tf.random_uniform(shape=[32, 10]) * vocab_size, tf.int32) # bs x seq_len\n",
    "mask = tf.cast(tf.random_uniform(shape=[32, 10]) * 2, tf.int32) # bs x seq_len\n",
    "inputs_len = tf.reduce_sum(mask, axis=1)\n",
    "embedding_matrix = tf.random_uniform(shape=[vocab_size, hidden_dim])\n",
    "\n",
    "encoder(inputs, inputs_len, embedding_matrix, hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder\n",
    "Decoder uses encoder outputs and encoder state to produce output sequence.\n",
    "\n",
    "Here, you should:\n",
    "* define your decoder_cell (GRU or LSTM)\n",
    "\n",
    "it will be your baseline seq2seq model.\n",
    "\n",
    "\n",
    "And, to improve the model:\n",
    "* add Teacher Forcing\n",
    "* add Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(encoder_outputs, encoder_state, embedding_matrix, mask,\n",
    "            cell_size, max_length, y_ph,\n",
    "            start_token_id=1, keep_prob=1.0,\n",
    "            teacher_forcing_rate_ph=None,\n",
    "            use_attention=False, is_train=True):\n",
    "    # decoder\n",
    "    # encoder_outputs: tf.float32 tensor with shape bs x seq_len x encoder_cell_size\n",
    "    # encoder_state: tf.float32 tensor with shape bs x encoder_cell_size\n",
    "    # embedding_matrix: tf.float32 tensor with shape vocab_size x vocab_dim\n",
    "    # mask: tf.int32 tensor with shape bs x seq_len with zeros for masked sequence elements\n",
    "    # cell_size: hidden size of recurrent cell\n",
    "    # max_length: max length of output sequence\n",
    "    # start_token_id: id of <BOS> token in vocabulary\n",
    "    # keep_prob: dropout keep probability\n",
    "    # teacher_forcing_rate_ph: rate of using teacher forcing on each decoding step\n",
    "    # use_attention: use attention on encoder outputs or use only encoder_state\n",
    "    # is_train: is it training or inference? at inference time we can't use teacher forcing\n",
    "    with tf.variable_scope('decoder'):\n",
    "        # define decoder recurrent cell\n",
    "        decoder_cell = tf.nn.rnn_cell.GRUCell(\n",
    "                            num_units=cell_size,\n",
    "                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                            name='decoder_cell')\n",
    "        \n",
    "        # initial value of output_token on previsous step is start_token\n",
    "        output_token = tf.ones(shape=(tf.shape(encoder_outputs)[0],), dtype=tf.int32) * start_token_id\n",
    "        # let's define initial value of decoder state with encoder_state\n",
    "        decoder_state = encoder_state\n",
    "\n",
    "        pred_tokens = []\n",
    "        logits = []\n",
    "\n",
    "        # use for loop to sequentially call recurrent cell\n",
    "        for i in range(max_length):\n",
    "            \"\"\"\n",
    "            TEACHER FORCING\n",
    "            # here you can try to implement teacher forcing for your model\n",
    "            # details about teacher forcing are explained further in tutorial\n",
    "            \n",
    "            # pseudo code:\n",
    "            NOTE THAT FOLLOWING CONDITIONS SHOULD BE EVALUATED AT GRAPH RUNTIME\n",
    "            use tf.cond and tf.logical operations instead of python if\n",
    "            \n",
    "            if i > 0 and is_train and random_value < teacher_forcing_rate_ph:\n",
    "                input_token = y_ph[:, i-1]\n",
    "            else:\n",
    "                input_token = output_token\n",
    "\n",
    "            input_token_emb = tf.nn.embedding_lookup(embedding_matrix, input_token)\n",
    "            \n",
    "            \"\"\"\n",
    "            if i > 0:\n",
    "                input_token_emb = tf.cond(\n",
    "                                      tf.logical_and(\n",
    "                                          is_train,\n",
    "                                          tf.random_uniform(shape=(), maxval=1) <= teacher_forcing_rate_ph\n",
    "                                      ),\n",
    "                                      lambda: tf.nn.embedding_lookup(embedding_matrix, y_ph[:, i-1]), # teacher forcing\n",
    "                                      lambda: tf.nn.embedding_lookup(embedding_matrix, output_token)\n",
    "                                      )\n",
    "            else:\n",
    "                input_token_emb = tf.nn.embedding_lookup(embedding_matrix, output_token)\n",
    "\n",
    "            \"\"\"\n",
    "            ATTENTION MECHANISM\n",
    "            # here you can add attention to your model\n",
    "            # you can find details about attention further in tutorial\n",
    "            \"\"\"            \n",
    "            if use_attention:\n",
    "                # compute attention and concat attention vector to input_token_emb\n",
    "                att = dot_attention(encoder_outputs, decoder_state, mask, scope='att')\n",
    "                input_token_emb = tf.concat([input_token_emb, att], axis=-1)\n",
    "\n",
    "\n",
    "            input_token_emb = tf.nn.dropout(input_token_emb, keep_prob=keep_prob)\n",
    "            # call recurrent cell\n",
    "            decoder_outputs, decoder_state = decoder_cell(input_token_emb, decoder_state)\n",
    "            decoder_outputs = tf.nn.dropout(decoder_outputs, keep_prob=keep_prob)\n",
    "            # project decoder output to embeddings dimension\n",
    "            embeddings_dim = embedding_matrix.get_shape()[1]\n",
    "            output_proj = tf.layers.dense(decoder_outputs, embeddings_dim, activation=tf.nn.tanh,\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          name='proj', reuse=tf.AUTO_REUSE)\n",
    "            # compute logits\n",
    "            output_logits = tf.matmul(output_proj, embedding_matrix, transpose_b=True)\n",
    "\n",
    "            logits.append(output_logits)\n",
    "            output_probs = tf.nn.softmax(output_logits)\n",
    "            output_token = tf.argmax(output_probs, axis=-1)\n",
    "            pred_tokens.append(output_token)\n",
    "\n",
    "        y_pred_tokens = tf.transpose(tf.stack(pred_tokens, axis=0), [1, 0])\n",
    "        y_logits = tf.transpose(tf.stack(logits, axis=0), [1, 0, 2])\n",
    "    return y_pred_tokens, y_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output of next cell should be with shapes:\n",
    "\n",
    "    32 x 10\n",
    "    32 x 10 x 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "vocab_size = 100\n",
    "hidden_dim = 100\n",
    "inputs = tf.cast(tf.random_uniform(shape=[32, 10]) * vocab_size, tf.int32) # bs x seq_len\n",
    "mask = tf.cast(tf.random_uniform(shape=[32, 10]) * 2, tf.int32) # bs x seq_len\n",
    "inputs_len = tf.reduce_sum(mask, axis=1)\n",
    "embedding_matrix = tf.random_uniform(shape=[vocab_size, hidden_dim])\n",
    "\n",
    "teacher_forcing_rate = tf.random_uniform(shape=())\n",
    "y = tf.cast(tf.random_uniform(shape=[32, 10]) * vocab_size, tf.int32)\n",
    "\n",
    "encoder_outputs, encoder_state = encoder(inputs, inputs_len, embedding_matrix, hidden_dim)\n",
    "decoder(encoder_outputs, encoder_state, embedding_matrix, mask, hidden_dim, max_length=10,\n",
    "        y_ph=y, teacher_forcing_rate_ph=teacher_forcing_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2Seq model should be inherited from TFModel class and implement following methods:\n",
    "* train_on_batch - this method is called in training phase\n",
    "* \\_\\_call\\_\\_ - this method is called to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.models.tf_model import TFModel\n",
    "\n",
    "@register('seq2seq')\n",
    "class Seq2Seq(TFModel):\n",
    "    def __init__(self, **kwargs):\n",
    "        # hyperparameters\n",
    "        \n",
    "        # dimension of word embeddings\n",
    "        self.embeddings_dim = kwargs.get('embeddings_dim', 100)\n",
    "        # size of recurrent cell in encoder and decoder\n",
    "        self.cell_size = kwargs.get('cell_size', 200)\n",
    "        # dropout keep_probability\n",
    "        self.keep_prob = kwargs.get('keep_prob', 0.8)\n",
    "        # learning rate\n",
    "        self.learning_rate = kwargs.get('learning_rate', 3e-04)\n",
    "        # max length of output sequence\n",
    "        self.max_length = kwargs.get('max_length', 20)\n",
    "        self.grad_clip = kwargs.get('grad_clip', 5.0)\n",
    "        self.start_token_id = kwargs.get('start_token_id', 1)\n",
    "        self.vocab_size = kwargs.get('vocab_size', 11595)\n",
    "        self.teacher_forcing_rate = kwargs.get('teacher_forcing_rate', 0.0)\n",
    "        self.use_attention = kwargs.get('use_attention', False)\n",
    "        \n",
    "        # create tensorflow session to run computational graph in it\n",
    "        self.sess_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "        self.sess_config.gpu_options.allow_growth = True\n",
    "        self.sess = tf.Session(config=self.sess_config)\n",
    "        \n",
    "        self.init_graph()\n",
    "        \n",
    "        # define train op\n",
    "        self.train_op = self.get_train_op(self.loss, self.lr_ph,\n",
    "                                          optimizer=tf.train.AdamOptimizer,\n",
    "                                          clip_norm=self.grad_clip)\n",
    "        # initialize graph variables\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "        # load saved model if there is one\n",
    "        if self.load_path is not None:\n",
    "            self.load()\n",
    "        \n",
    "    def init_graph(self):\n",
    "        # create placeholders\n",
    "        self.init_placeholders()\n",
    "\n",
    "        self.x_mask = tf.cast(self.x_ph, tf.int32) \n",
    "        self.y_mask = tf.cast(self.y_ph, tf.int32) \n",
    "        \n",
    "        self.x_len = tf.reduce_sum(self.x_mask, axis=1)\n",
    "        \n",
    "        # create embeddings matrix for tokens\n",
    "        self.embeddings = tf.Variable(tf.random_uniform((self.vocab_size, self.embeddings_dim), -0.1, 0.1, name='embeddings'), dtype=tf.float32)\n",
    "\n",
    "        # encoder\n",
    "        encoder_outputs, encoder_state = encoder(self.x_ph, self.x_len, self.embeddings, self.cell_size, self.keep_prob_ph)\n",
    "\n",
    "        # decoder\n",
    "        self.y_pred_tokens, y_logits = decoder(encoder_outputs, encoder_state, self.embeddings, self.x_mask,\n",
    "                                                      self.cell_size, self.max_length,\n",
    "                                                      self.y_ph, self.start_token_id, self.keep_prob_ph,\n",
    "                                                      self.teacher_forcing_rate_ph, self.use_attention, self.is_train_ph)\n",
    "        \n",
    "        # loss\n",
    "        self.y_ohe = tf.one_hot(self.y_ph, depth=self.vocab_size)\n",
    "        self.y_mask = tf.cast(self.y_mask, tf.float32)\n",
    "        self.loss = tf.nn.softmax_cross_entropy_with_logits(labels=self.y_ohe, logits=y_logits) * self.y_mask\n",
    "        self.loss = tf.reduce_sum(self.loss) / tf.reduce_sum(self.y_mask)\n",
    "    \n",
    "    def init_placeholders(self):\n",
    "        # placeholders for inputs\n",
    "        self.x_ph = tf.placeholder(shape=(None, None), dtype=tf.int32, name='x_ph')\n",
    "        # at inference time y_ph is used (y_ph exists in computational graph)  when teacher forcing is activated, so we add dummy default value\n",
    "        # this dummy value is not actually used at inference\n",
    "        self.y_ph = tf.placeholder_with_default(tf.zeros_like(self.x_ph), shape=(None,None), name='y_ph')\n",
    "\n",
    "        # placeholders for model parameters\n",
    "        self.lr_ph = tf.placeholder(dtype=tf.float32, shape=[], name='lr_ph')\n",
    "        self.keep_prob_ph = tf.placeholder_with_default(1.0, shape=[], name='keep_prob_ph')\n",
    "        self.is_train_ph = tf.placeholder_with_default(False, shape=[], name='is_train_ph')\n",
    "        self.teacher_forcing_rate_ph = tf.placeholder_with_default(0.0, shape=[], name='teacher_forcing_rate_ph')\n",
    "            \n",
    "    def _build_feed_dict(self, x, y=None):\n",
    "        feed_dict = {\n",
    "            self.x_ph: x,\n",
    "        }\n",
    "        if y is not None:\n",
    "            feed_dict.update({\n",
    "                self.y_ph: y,\n",
    "                self.lr_ph: self.learning_rate,\n",
    "                self.keep_prob_ph: self.keep_prob,\n",
    "                self.is_train_ph: True,\n",
    "                self.teacher_forcing_rate_ph: self.teacher_forcing_rate,\n",
    "            })\n",
    "        return feed_dict\n",
    "    \n",
    "    def train_on_batch(self, x, y):\n",
    "        feed_dict = self._build_feed_dict(x, y)\n",
    "        loss, _ = self.sess.run([self.loss, self.train_op], feed_dict=feed_dict)\n",
    "        return loss\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        feed_dict = self._build_feed_dict(x)\n",
    "        y_pred = self.sess.run(self.y_pred_tokens, feed_dict=feed_dict)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create model with random weights and default parameters, change path to model, otherwise it will be stored in deeppavlov/download folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2s = Seq2Seq(\n",
    "    save_path='./seq2seq_model',\n",
    "    load_path='./seq2seq_model'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we firstly run all preprocessing steps and call seq2seq model, and then convert token indexes to tokens. As result we should get some random sequence of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab(s2s(padder(vocab([['hello', 'my', 'friend', 'there_is_no_such_word_in_dataset', 'and_this']]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention mechanism\n",
    "Attention mechanism [[paper](https://arxiv.org/abs/1409.0473)] allows to aggregate information from \"memory\" according to current state. By aggregating we suppose weighted sum of \"memory\" items. Weight of each memory item depends on current state.\n",
    "\n",
    "Without attention decoder could use only last hidden state of encoder. Attention mechanism gives access to all encoder states during decoding.\n",
    "\n",
    "![attention](img/attention.png)\n",
    "\n",
    "One of the simpliest ways to compute attention weights (*a_ij*) is to compute them by dot product between memory items and state and then apply softmax function. Other ways of computing *multiplicative* attention could be found in this [paper](https://arxiv.org/abs/1508.04025).\n",
    "\n",
    "We also need a mask to skip some sequence elements like <PAD\\>. To make weight of undesired memory items close to zero we can add big negative value to logits (result of dot product) before applying softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_mask(values, mask):\n",
    "    # adds big negative to masked values\n",
    "    INF = 1e30\n",
    "    return -INF * (1 - tf.cast(mask, tf.float32)) + values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_attention(memory, state, mask, scope=\"dot_attention\"):\n",
    "    # inputs: bs x seq_len x hidden_dim\n",
    "    # state: bs x hidden_dim\n",
    "    # mask: bs x seq_len\n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        # dot product between each item in memory and state\n",
    "        logits = tf.matmul(memory, tf.expand_dims(state, axis=1), transpose_b=True)\n",
    "        logits = tf.squeeze(logits, [2])\n",
    "        \n",
    "        # apply mask to logits\n",
    "        logits = softmax_mask(logits, mask)\n",
    "        \n",
    "        # apply softmax to logits\n",
    "        att_weights = tf.expand_dims(tf.nn.softmax(logits), axis=2)\n",
    "        \n",
    "        # compute weighted sum of items in memory\n",
    "        att = tf.reduce_sum(att_weights * memory, axis=1)\n",
    "        return att"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your implementation:\n",
    "\n",
    "outputs should be with shapes 32 x 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "memory = tf.random_normal(shape=[32, 10, 100]) # bs x seq_len x hidden_dim\n",
    "state = tf.random_normal(shape=[32, 100]) # bs x hidden_dim\n",
    "mask = tf.cast(tf.random_normal(shape=[32, 10]), tf.int32) # bs x seq_len\n",
    "dot_attention(memory, state, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teacher forcing\n",
    "\n",
    "We have implemented decoder, which takes as input it's own output during training and inference time. But, at early stages of training it could be hard for model to produce long sequences depending on it's own close to random output. Teacher forcing can help with this: instead of feeding model's output we can feed ground truth tokens. It helps model on training time, but on inference we still can rely only on it's own output.\n",
    "\n",
    "\n",
    "Using model's output:\n",
    "\n",
    "<img src=\"img/sampling.png\" alt=\"sampling\" width=50%/>\n",
    "\n",
    "Teacher forcing:\n",
    "\n",
    "<img src=\"img/teacher_forcing.png\" alt=\"teacher_forcing\" width=50%/>\n",
    "\n",
    "It is not necessary to feed ground truth tokens on each time step - we can randomly choose with some rate if we want ground truth input or predicted by model.\n",
    "*teacher_forcing_rate* parameter of seq2seq model can control such behavior.\n",
    "\n",
    "More details about teacher forcing could be found in DeepLearningBook [Chapter 10.2.1](http://www.deeplearningbook.org/contents/rnn.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create model with random weights and default parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we firstly run all preprocessing steps and call seq2seq model, and then convert token indexes to tokens. As result we should get some random sequence of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In postprocessing step we are going to remove all <PAD\\>, <BOS\\>, <EOS\\> tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register('postprocessing')\n",
    "class SentencePostprocessor(Component):\n",
    "    def __init__(self, pad_token='<PAD>', start_token='<BOS>', end_token='<EOS>', *args, **kwargs):\n",
    "        self.pad_token = pad_token\n",
    "        self.start_token = start_token\n",
    "        self.end_token = end_token\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        for i in range(len(batch)):\n",
    "            batch[i] = ' '.join(self._postproc(batch[i]))\n",
    "        return batch\n",
    "    \n",
    "    def _postproc(self, utt):\n",
    "        if self.end_token in utt:\n",
    "            utt = utt[:utt.index(self.end_token)]\n",
    "        return utt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocess = SentencePostprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocess(vocab(s2s(padder(vocab([['hello', 'my', 'friend', 'there_is_no_such_word_in_dataset', 'and_this']])))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create config file\n",
    "Let's put is all together in one config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"dataset_reader\": {\n",
    "    \"class_name\": \"personachat_dataset_reader\",\n",
    "    \"data_path\": \"./personachat\"\n",
    "  },\n",
    "  \"dataset_iterator\": {\n",
    "    \"class_name\": \"personachat_iterator\",\n",
    "    \"seed\": 1337,\n",
    "    \"shuffle\": True\n",
    "  },\n",
    "  \"chainer\": {\n",
    "    \"in\": [\"x\"],\n",
    "    \"in_y\": [\"y\"],\n",
    "    \"pipe\": [\n",
    "      {\n",
    "        \"class_name\": \"lazy_tokenizer\",\n",
    "        \"id\": \"tokenizer\",\n",
    "        \"in\": [\"x\"],\n",
    "        \"out\": [\"x_tokens\"]\n",
    "      },\n",
    "      {\n",
    "        \"class_name\": \"lazy_tokenizer\",\n",
    "        \"id\": \"tokenizer\",\n",
    "        \"in\": [\"y\"],\n",
    "        \"out\": [\"y_tokens\"]\n",
    "      },\n",
    "      {\n",
    "        \"class_name\": \"dialog_vocab\",\n",
    "        \"id\": \"vocab\",\n",
    "        \"save_path\": \"./vocab.dict\",\n",
    "        \"load_path\": \"./vocab.dict\",\n",
    "        \"min_freq\": 2,\n",
    "        \"special_tokens\": [\"<PAD>\",\"<BOS>\", \"<EOS>\", \"<UNK>\"],\n",
    "        \"unk_token\": \"<UNK>\",\n",
    "        \"fit_on\": [\"x_tokens\", \"y_tokens\"],\n",
    "        \"in\": [\"x_tokens\"],\n",
    "        \"out\": [\"x_tokens_ids\"]\n",
    "      },\n",
    "      {\n",
    "        \"ref\": \"vocab\",\n",
    "        \"in\": [\"y_tokens\"],\n",
    "        \"out\": [\"y_tokens_ids\"]\n",
    "      },\n",
    "      {\n",
    "        \"class_name\": \"sentence_padder\",\n",
    "        \"id\": \"padder\",\n",
    "        \"length_limit\": 20,\n",
    "        \"in\": [\"x_tokens_ids\"],\n",
    "        \"out\": [\"x_tokens_ids\"]\n",
    "      },\n",
    "      {\n",
    "        \"ref\": \"padder\",\n",
    "        \"in\": [\"y_tokens_ids\"],\n",
    "        \"out\": [\"y_tokens_ids\"]\n",
    "      },\n",
    "      {\n",
    "        \"class_name\": \"seq2seq\",\n",
    "        \"id\": \"s2s\",\n",
    "        \"max_length\": \"#padder.length_limit+2\",\n",
    "        \"cell_size\": 250,\n",
    "        \"embeddings_dim\": 50,\n",
    "        \"vocab_size\": 11595,\n",
    "        \"keep_prob\": 0.8,\n",
    "        \"learning_rate\": 3e-04,\n",
    "        \"teacher_forcing_rate\": 0.0,\n",
    "        \"use_attention\": False,\n",
    "        \"save_path\": \"./seq2seq_model\",\n",
    "        \"load_path\": \"./seq2seq_model\",\n",
    "        \"in\": [\"x_tokens_ids\"],\n",
    "        \"in_y\": [\"y_tokens_ids\"],\n",
    "        \"out\": [\"y_predicted_tokens_ids\"],\n",
    "      },\n",
    "      {\n",
    "        \"ref\": \"vocab\",\n",
    "        \"in\": [\"y_predicted_tokens_ids\"],\n",
    "        \"out\": [\"y_predicted_tokens\"]\n",
    "      },\n",
    "      {\n",
    "        \"class_name\": \"postprocessing\",\n",
    "        \"in\": [\"y_predicted_tokens\"],\n",
    "        \"out\": [\"y_predicted_tokens\"]\n",
    "      }\n",
    "    ],\n",
    "    \"out\": [\"y_predicted_tokens\"]\n",
    "  },\n",
    "  \"train\": {\n",
    "    \"log_every_n_batches\": 100,\n",
    "    \"val_every_n_epochs\":0,\n",
    "    \"batch_size\": 64,\n",
    "    \"validation_patience\": 0,\n",
    "    \"epochs\": 20,\n",
    "    \"metrics\": [\"bleu\"],\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with model using config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov import build_model\n",
    "model = build_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(['Hi, how are you?', 'Any ideas my dear friend?'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run experiments with and without attention, with teacher forcing and without."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.commands.train import train_evaluate_model_from_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_evaluate_model_from_config(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(config)\n",
    "model(['hi, how are you?', 'any ideas my dear friend?', 'okay, i agree with you', 'good bye!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the model you can try to use multilayer (use MultiRNNCell) encoder and decoder, try to use attention with trainable parameters (not dot product scoring function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
